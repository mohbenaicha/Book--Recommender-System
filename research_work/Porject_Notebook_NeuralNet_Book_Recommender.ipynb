{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ae282d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "import tensorflow_recommenders as tfrs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c2d22",
   "metadata": {},
   "source": [
    "Importing and organizing the raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0260233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# data = list()\n",
    "# for file in os.listdir(\"./data\"):\n",
    "##     if file != \"results_20000-24861.json\":\n",
    "#     with open(\"./data/\"+file, 'rb') as f:\n",
    "#         print(f\"Read {file}\")\n",
    "#         data_ = json.load(f)\n",
    "#         data += data_\n",
    "##     else:\n",
    "##         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee7f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# records = [{\"User\": record.get('user_id'), \n",
    "#            \"ISBN\": record.get('book_title'),\n",
    "#            \"RatingOf5\": record.get('stars'),\n",
    "#            \"Genres\": record.get('genres')\n",
    "#           } for record in data]\n",
    "# df = pd.DataFrame(records)\n",
    "# df.to_csv(\"USER_ISBN_RATING_GENRES.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e423556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"USER_ISBN_RATING_GENRES.csv\")\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bce96ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numer of unique books:  57198\n",
      "Total numer of unique users:  706880\n",
      "Total numer of reviews:  1733561\n",
      "Total numer of positive reviews:  936796\n",
      "Total numer of non-positive reviews:  201628\n",
      "Total numer of moderate reviews:  372374\n",
      "Total numer of  non-reviews:  222763\n"
     ]
    }
   ],
   "source": [
    "print(\"Total numer of unique books: \", len(df[\"ISBN\"].unique()))\n",
    "print(\"Total numer of unique users: \", len(df[\"User\"].unique()))\n",
    "print(\"Total numer of reviews: \", len(df))\n",
    "print(\"Total numer of positive reviews: \", len(df[df[\"RatingOf5\"] > 3]))\n",
    "print(\"Total numer of non-positive reviews: \", len(df[df[\"RatingOf5\"] < 3]))\n",
    "print(\"Total numer of moderate reviews: \", len(df[df[\"RatingOf5\"] == 3]))\n",
    "print(\"Total numer of  non-reviews: \", sum(df[\"RatingOf5\"].isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45c216",
   "metadata": {},
   "source": [
    "To keep users anonymous, I'll label encode them as IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "129056e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1733561, 706880)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df[\"User\"])\n",
    "transformed = le.transform(df[\"User\"])\n",
    "len(transformed), len(np.unique(transformed, return_counts=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "efc5427f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fitted_label_encoder.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(le, \"fitted_label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c9afe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = joblib.load(\"fitted_label_encoder.pkl\")\n",
    "transformed = le.transform(df[\"User\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6649eba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    423114\n",
       "1    241063\n",
       "2    659623\n",
       "3    557019\n",
       "4     44930\n",
       "Name: User, dtype: int32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"User\"] = transformed\n",
    "df[\"User\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "848a364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf1817c",
   "metadata": {},
   "source": [
    "**I'm going to reserve a random sample of 10 observations to test recommendations as observations that the model has not seen before when developing the production package.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5ea0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.groupby(\"ISBN\").count()[df.groupby(\"ISBN\").count()[\"User\"] > 5][\"User\"].sort_values(ascending=False).sample(20).index\n",
    "loop = True\n",
    "while True:\n",
    "    new_sample=df[df[\"ISBN\"].isin(sample)].sample(10)['ISBN']\n",
    "    if len(new_sample.unique()) == 10:\n",
    "        sample_to_keep = new_sample\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03f41a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df[df.index.isin(sample_to_keep.index)][['User', 'ISBN']]\n",
    "index_to_drop = sample.index\n",
    "sample.to_csv('test.csv', index=False)\n",
    "df.drop(index=index_to_drop, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "24e9dd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>RatingOf5</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [User, ISBN, RatingOf5, Genres]\n",
       "Index: []"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.index.isin(index_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb651465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>ISBN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>505923</td>\n",
       "      <td>Veggie Lovers Cook Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>458816</td>\n",
       "      <td>Candyland: A Novel in Two Parts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188134</td>\n",
       "      <td>Urban Horrors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>678813</td>\n",
       "      <td>Trieste and The Meaning of Nowhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>237343</td>\n",
       "      <td>The Silver Stallion: A Comedy of Redemption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>627567</td>\n",
       "      <td>In meinem Himmel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97299</td>\n",
       "      <td>Kipper's Game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30625</td>\n",
       "      <td>Amy Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>312942</td>\n",
       "      <td>Mom Among the Liars Mom 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>408986</td>\n",
       "      <td>Jamberry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     User                                         ISBN\n",
       "0  505923                      Veggie Lovers Cook Book\n",
       "1  458816              Candyland: A Novel in Two Parts\n",
       "2  188134                                Urban Horrors\n",
       "3  678813           Trieste and The Meaning of Nowhere\n",
       "4  237343  The Silver Stallion: A Comedy of Redemption\n",
       "5  627567                             In meinem Himmel\n",
       "6   97299                                Kipper's Game\n",
       "7   30625                                     Amy Girl\n",
       "8  312942                    Mom Among the Liars Mom 4\n",
       "9  408986                                     Jamberry"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1a738f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"User\", \"ISBN\"]].to_csv('train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a990170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>ISBN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423114</td>\n",
       "      <td>Kiss Hollywood Goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>241063</td>\n",
       "      <td>Kiss Hollywood Goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>659623</td>\n",
       "      <td>Kiss Hollywood Goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>557019</td>\n",
       "      <td>Kiss Hollywood Goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44930</td>\n",
       "      <td>Kiss Hollywood Goodbye</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     User                    ISBN\n",
       "0  423114  Kiss Hollywood Goodbye\n",
       "1  241063  Kiss Hollywood Goodbye\n",
       "2  659623  Kiss Hollywood Goodbye\n",
       "3  557019  Kiss Hollywood Goodbye\n",
       "4   44930  Kiss Hollywood Goodbye"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('train_data.csv', nrows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc6642",
   "metadata": {},
   "source": [
    "#### Reducing dimentionality\n",
    "\n",
    "With ~ 700k users and 60k books, the embeddings for the two embedding \"towers\" that the recommender model is going to be based on will generately an extremely large model. Practically, I would like to demonstrate the end-to-end data science pipeline by being able to package my model and make it available publicly for my API to access as well as for those interested which is why reduce the recommendations to those users who given more than 2 review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af78f7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User\n",
       "0         42\n",
       "7          3\n",
       "8          4\n",
       "25         3\n",
       "31         3\n",
       "          ..\n",
       "706863    12\n",
       "706867     6\n",
       "706870     6\n",
       "706871     4\n",
       "706879     4\n",
       "Name: ISBN, Length: 130888, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data = df.groupby(\"User\").count()\n",
    "grouped_data[grouped_data[\"ISBN\"] > 2]['ISBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9516c478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1733551"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dbffddbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_keep = grouped_data[grouped_data[\"ISBN\"] > 2].index\n",
    "set(idx_to_keep) == set(df[df[\"User\"].isin(idx_to_keep)][\"User\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb70607",
   "metadata": {},
   "source": [
    "At this point we have an index of user that have more than 2 reviews (of different books). Our new DataFrame object will only comprise those users and their respectively reviewed books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e217c46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>ISBN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>732896</th>\n",
       "      <td>214798</td>\n",
       "      <td>The Ooze Ghosts of Fear Street 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134638</th>\n",
       "      <td>184939</td>\n",
       "      <td>The Search Cooper's Corner 13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701311</th>\n",
       "      <td>102597</td>\n",
       "      <td>Crossing Antarctica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469864</th>\n",
       "      <td>124818</td>\n",
       "      <td>Rules of the Hunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716250</th>\n",
       "      <td>334105</td>\n",
       "      <td>Mrs Dalloway</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           User                              ISBN\n",
       "732896   214798  The Ooze Ghosts of Fear Street 8\n",
       "134638   184939     The Search Cooper's Corner 13\n",
       "1701311  102597               Crossing Antarctica\n",
       "1469864  124818                 Rules of the Hunt\n",
       "1716250  334105                      Mrs Dalloway"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.User.isin(idx_to_keep)]\n",
    "df.sample(5)[['User', 'ISBN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a90e802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique users remaining:  130888\n",
      "# of unique books remaining:  56669\n"
     ]
    }
   ],
   "source": [
    "# also ensure that the grouping and querying was sane\n",
    "print(\"# of unique users remaining: \", len(df.groupby('User').count()))\n",
    "\n",
    "# also check out how many unique books were lost in the process of trimming \n",
    "# down the dataset (only about 500)\n",
    "\n",
    "print(\"# of unique books remaining: \", len(df.groupby('ISBN').count()))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99e941df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can label encode the book ID's if we so wish\n",
    "le_isbn = LabelEncoder()\n",
    "df[\"ISBN\"] = le_isbn.fit_transform(df[\"ISBN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348882f3",
   "metadata": {},
   "source": [
    "Setting up tensors to batch for purposes of training the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a3689c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = tf.data.Dataset.from_tensor_slices(\n",
    "    df[\"ISBN\"].astype('str').values).shuffle(128) # TODO: config (shuffle seed + size)\n",
    "\n",
    "ratings = tf.data.Dataset.from_tensor_slices(\n",
    "    df[[\"ISBN\", \"User\"]].astype('str').values).shuffle(128) # TODO: config (shuffle seed + size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5fb66023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['25879', '339182'],\n",
       " ['51897', '638932'],\n",
       " ['22378', '569243'],\n",
       " ['50409', '445633'],\n",
       " ['25879', '487228'],\n",
       " ['22378', '423114'],\n",
       " ['14058', '197397'],\n",
       " ['22378', '16953'],\n",
       " ['32867', '111877'],\n",
       " ['46288', '667545']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in ratings.batch(batch_size=10).take(1):\n",
    "    break\n",
    "\n",
    "# [[Book title, User ID],]\n",
    "[[batch[i].numpy()[j].decode() for j in range(2)] for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae1f6b",
   "metadata": {},
   "source": [
    "String look up creates a vocabulary dictionary of string-value pairs since I'll be embedding my ISBN's and users ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c94c80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x: x[1]))\n",
    "\n",
    "book_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "book_titles_vocabulary.adapt(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "411626e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"user_ids_vocabulary\", user_ids_vocabulary.get_weights())\n",
    "np.save(\"book_titles_vocabulary\", book_titles_vocabulary.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d0f74510",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_book_title_stringlookup = tf.keras.layers.StringLookup(mask_token=None)\n",
    "new_book_title_stringlookup.set_weights(np.load(\"book_titles_vocabulary.npy\", allow_pickle=True))\n",
    "new_user_id_stringlookup = tf.keras.layers.StringLookup(mask_token=None)\n",
    "new_user_id_stringlookup.set_weights(np.load(\"user_ids_vocabulary.npy\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8896f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique User ID counts:  130888 130889\n",
      "Unique ISBN counts:  56669 56670\n"
     ]
    }
   ],
   "source": [
    "# compare vocab against original data as a sanity check\n",
    "print(\"Unique User ID counts: \", len(df[\"User\"].unique()), new_user_id_stringlookup.vocabulary_size())\n",
    "print(\"Unique ISBN counts: \", len(df[\"ISBN\"].unique()), new_book_title_stringlookup.vocabulary_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f100ec6d",
   "metadata": {},
   "source": [
    "The difference in \"vocab\" counts results from the [UNK] (i.e. unknown) token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4276c",
   "metadata": {},
   "source": [
    "Testing stringlookup on the saved sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e99c6e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56670"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_book_title_stringlookup.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "206e0238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)>,\n",
       " 59625                          Veggie Lovers Cook Book\n",
       " 292819                 Candyland: A Novel in Two Parts\n",
       " 356947                                   Urban Horrors\n",
       " 481544              Trieste and The Meaning of Nowhere\n",
       " 695979     The Silver Stallion: A Comedy of Redemption\n",
       " 805528                                In meinem Himmel\n",
       " 1152083                                  Kipper's Game\n",
       " 1340451                                       Amy Girl\n",
       " 1465684                      Mom Among the Liars Mom 4\n",
       " 1527060                                       Jamberry\n",
       " Name: ISBN, dtype: object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_book_title_stringlookup.call(tf.constant(sample['ISBN'].astype(str))), sample['ISBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1b54208e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=int64, numpy=\n",
       " array([ 99278, 102845,    181,  45465,  28074,   2431,   6463, 114522,\n",
       "           231,  19681], dtype=int64)>,\n",
       " 59625      505923\n",
       " 292819     458816\n",
       " 356947     188134\n",
       " 481544     678813\n",
       " 695979     237343\n",
       " 805528     627567\n",
       " 1152083     97299\n",
       " 1340451     30625\n",
       " 1465684    312942\n",
       " 1527060    408986\n",
       " Name: User, dtype: int32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_id_stringlookup.call(tf.constant(sample['User'].astype(str))), sample['User']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0c9cc",
   "metadata": {},
   "source": [
    "Several user_ids have a string lookup of 0 which stands for unknown meaning they were likely dropped during the attemp at dimentionality reduction (i.e. these users had only ever reviewed one/two book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "63866a0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [101]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m idx \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# this is a bit of  confusing one-liner that simply attempts to search for the indices of users that \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# had the string lookup assign them a value of 0 and thus are the suspicions confirmed that\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# they've been dropped during the dimentionality reduction step since they're users with less than \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 2 total reviews.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df[df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(sample[sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(idx)]\u001b[38;5;241m.\u001b[39mindex)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx = [0, 3, 8]\n",
    "\n",
    "# this is a bit of  confusing one-liner that simply attempts to search for the indices of users that \n",
    "# had the string lookup assign them a value of 0 and thus are the suspicions confirmed that\n",
    "# they've been dropped during the dimentionality reduction step since they're users with less than \n",
    "# 2 total reviews.\n",
    "\n",
    "assert len(df[df.index.isin(sample[sample['User'].reset_index().index.isin(idx)].index)]['User'].unique()) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873a44d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "I'll be doing tests for label encoders and string lookups on the books columns and not the users columns before deployment \n",
    "since their info is confidential, otherwise, for testing purposes, the sample taken isn't valid and would fail the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9e09f",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d073e7d7",
   "metadata": {},
   "source": [
    "The following class enherits from the TensorFlow Recommenders class and sets up the embedding models for users and books that will be fit to reduce loss. This is also referred to as the two tower model since the embeddings are very long along one dimension (that of the users/books) and shorter on the other (the features dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2853d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024*8\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7d4d98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(tfrs.Model):\n",
    "    def __init__(\n",
    "      self,\n",
    "      user_model: tf.keras.Model,\n",
    "      book_model: tf.keras.Model,\n",
    "      task: tfrs.tasks.Retrieval):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "    \n",
    "        # embedding representations\n",
    "        self.user_model = user_model\n",
    "        self.book_model = book_model\n",
    "        # this will receive the computed losses by using the retrieval class to calculate the FactorizedTopK\n",
    "        self.task = task \n",
    "    \n",
    "    def compute_loss(self, features, training=False) -> tf.Tensor:\n",
    "        # Define how the loss is computed.\n",
    "\n",
    "        user_embeddings = self.user_model(features[1])\n",
    "        book_embeddings = self.book_model(features[0])\n",
    "\n",
    "        return self.task(user_embeddings, book_embeddings, compute_metrics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c634ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "    new_user_id_stringlookup,\n",
    "    tf.keras.layers.Embedding(new_user_id_stringlookup.vocabulary_size()+1 , 32)\n",
    "])\n",
    "\n",
    "book_model = tf.keras.Sequential([\n",
    "    new_book_title_stringlookup,\n",
    "    tf.keras.layers.Embedding(new_book_title_stringlookup.vocabulary_size()+1, 32)\n",
    "])\n",
    "\n",
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "    books.batch(256).map(book_model)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "537c0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_model = TwoTowerModel(user_model, book_model, task)\n",
    "tt_model.compile(optimizer=tf.keras.optimizers.Adam(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "65bd120e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "129/129 [==============================] - 5s 35ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 0.8718 - regularization_loss: 0.0000e+00 - total_loss: 0.8718\n",
      "Epoch 2/3\n",
      "129/129 [==============================] - 5s 36ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 0.8277 - regularization_loss: 0.0000e+00 - total_loss: 0.8277\n",
      "Epoch 3/3\n",
      "129/129 [==============================] - 5s 35ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 0.8050 - regularization_loss: 0.0000e+00 - total_loss: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21c91580a30>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_model.fit(ratings.batch(BATCH_SIZE), epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ba059",
   "metadata": {},
   "source": [
    "The brute force approach goes through every index to get the top K recommendation. This is considered tedious and the alternative scalable nearest neighbours approach provides a viable alternative since the latter utilizes compressed vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "96c58ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x21c91583610>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = tfrs.layers.factorized_top_k.BruteForce(tt_model.user_model, k = 100)\n",
    "indexer.index_from_dataset(\n",
    "    books.batch(64).map(lambda title: (title, tt_model.book_model(title))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474dac1",
   "metadata": {},
   "source": [
    "Let's get some recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4fe8bc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 8]\n",
      "[505923 458816 188134 678813 237343 627567  97299  30625 312942 408986]\n"
     ]
    }
   ],
   "source": [
    "print(idx) # users that the string lookupand hence model doesn't know and should give then random recommendations\n",
    "print(sample['User'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "91f8b6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.72 ms ± 118 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'18684', '28325', '32678', '44154', '5252'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit _, titles = indexer(tf.constant([\"703730\"]))\n",
    "_, titles = indexer(tf.constant([\"703730\"]))\n",
    "set([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a4d08804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'52727', '53615'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, titles = indexer(tf.constant([\"3\"]))\n",
    "set([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e1945",
   "metadata": {},
   "source": [
    "The time taken is quite slow for a live model which is why a ScaNN alogorithm can be used which approximates recommendations user a nearest-neighbours approach.\n",
    "\n",
    "As a sanity check, I'll get a couple of unknow users' recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aaa0e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\"683095\" or \"47561\") not in new_user_id_stringlookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b75f52cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683095 :  {'52727', '53615'}\n",
      "47561  :  {'52727', '53615'}\n"
     ]
    }
   ],
   "source": [
    "for user in ['683095', '47561 ']:\n",
    "    _, titles = indexer(tf.constant([user]))\n",
    "    print(user, \": \", set([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5cd9e",
   "metadata": {},
   "source": [
    "The similarity in recommendations to unknown indicates that all unknown users are given the same random recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f77917",
   "metadata": {},
   "source": [
    "**Making sense of the recommendations requires analyzing the genres of book user \"100023\" read and liked.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a6e9baa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gothic',\n",
       " 'Dogs',\n",
       " 'Adventure',\n",
       " 'Suspense',\n",
       " 'Young Adult',\n",
       " 'Womens',\n",
       " 'Banned Books',\n",
       " 'How To',\n",
       " 'Nonfiction',\n",
       " 'Crime',\n",
       " 'Mystery Thriller',\n",
       " 'Family',\n",
       " 'Memoir',\n",
       " 'Short Story Collection',\n",
       " 'Womens Fiction',\n",
       " 'Ireland',\n",
       " 'Poetry',\n",
       " 'Philosophy',\n",
       " 'Adult',\n",
       " 'Health',\n",
       " 'Outdoors',\n",
       " 'The United States Of America',\n",
       " 'European Literature',\n",
       " '20th Century',\n",
       " 'Supernatural',\n",
       " 'Detective',\n",
       " 'France',\n",
       " 'Personal Development',\n",
       " 'Nature',\n",
       " 'Witches',\n",
       " 'Fiction',\n",
       " 'Kids',\n",
       " 'Scotland',\n",
       " 'Book Club',\n",
       " 'Cultural',\n",
       " 'Realistic Fiction',\n",
       " 'Young Readers',\n",
       " 'Canada',\n",
       " 'Paranormal',\n",
       " 'Modern Classics',\n",
       " 'Animals',\n",
       " 'Academic',\n",
       " 'Travel',\n",
       " 'Coming Of Age',\n",
       " 'Female Authors',\n",
       " 'Chapter Books',\n",
       " 'School',\n",
       " 'Gothic Romance',\n",
       " 'Short Stories',\n",
       " 'Psychological Thriller',\n",
       " 'Juvenile',\n",
       " 'Cozy Mystery',\n",
       " 'Emergency Services',\n",
       " 'Audiobook',\n",
       " 'Spirituality',\n",
       " 'Self Help',\n",
       " 'Childrens',\n",
       " 'Historical',\n",
       " 'Autobiography',\n",
       " 'Diets',\n",
       " 'Psychology',\n",
       " 'Mystery',\n",
       " 'Biography Memoir',\n",
       " 'Romantic Suspense',\n",
       " 'Biography',\n",
       " 'Classics',\n",
       " 'Contemporary',\n",
       " 'Judaism',\n",
       " 'Buddhism',\n",
       " 'Romance',\n",
       " 'Novels',\n",
       " 'Literature',\n",
       " 'Irish Literature',\n",
       " 'Literary Fiction',\n",
       " 'British Literature',\n",
       " 'Feminism',\n",
       " 'Middle Grade',\n",
       " 'Humor',\n",
       " 'Religion',\n",
       " 'Canadian Literature',\n",
       " 'History',\n",
       " 'Historical Fiction',\n",
       " 'Environment',\n",
       " 'Relationships',\n",
       " 'Police',\n",
       " 'Jewish',\n",
       " 'Adult Fiction',\n",
       " 'Food and Drink',\n",
       " 'Murder Mystery',\n",
       " 'China',\n",
       " 'American',\n",
       " 'Thriller',\n",
       " 'Fantasy']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_genres = set()\n",
    "user_read_genres = df2[df2[\"User\"]==55862][\"Genres\"].unique()\n",
    "user_read_genres = [ast.literal_eval(user_read_genres[i]) for i in range(len(user_read_genres))];\n",
    "[[read_genres.add(i) for i in element] for element in user_read_genres];\n",
    "list(read_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d07fc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recommended_books = ['Brennen muss Salem',\n",
    " 'How to Know God: The Yoga Aphorisms of Patanjali',\n",
    " \"It's Not Mean If It's True: More Trials from My Queer Life\",\n",
    " 'Night Judgement At Sinos A Novel',\n",
    " 'The House on the Point: A Tribute to Franklin W Dixon and The Hardy Boys',\n",
    " 'Tularosa Kevin Kerney 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "68cdf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = df2[df2[\"ISBN\"].isin(user_recommended_books)]['Genres'].unique()\n",
    "recommendations = [ast.literal_eval(recommendations[i]) for i in range(len(recommendations))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a7c86487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Action',\n",
       " 'Adult',\n",
       " 'Adventure',\n",
       " 'Audiobook',\n",
       " 'Autobiography',\n",
       " 'Buddhism',\n",
       " 'Classics',\n",
       " 'Crime',\n",
       " 'Cultural',\n",
       " 'Eastern Philosophy',\n",
       " 'Emergency Services',\n",
       " 'Espionage',\n",
       " 'Essays',\n",
       " 'Fantasy',\n",
       " 'Fiction',\n",
       " 'Gay',\n",
       " 'Hinduism',\n",
       " 'Horror',\n",
       " 'Humor',\n",
       " 'India',\n",
       " 'LGBT',\n",
       " 'Memoir',\n",
       " 'Mystery',\n",
       " 'Mystery Thriller',\n",
       " 'Nonfiction',\n",
       " 'Novels',\n",
       " 'Paranormal',\n",
       " 'Philosophy',\n",
       " 'Police',\n",
       " 'Psychology',\n",
       " 'Queer',\n",
       " 'Religion',\n",
       " 'Spirituality',\n",
       " 'Spy Thriller',\n",
       " 'Supernatural',\n",
       " 'Suspense',\n",
       " 'Thriller',\n",
       " 'Vampires',\n",
       " 'Westerns',\n",
       " 'Writing'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_genres = set()\n",
    "[[rec_genres.add(i) for i in element] for element in recommendations];\n",
    "rec_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fe0ddc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adult',\n",
       " 'Adventure',\n",
       " 'Audiobook',\n",
       " 'Autobiography',\n",
       " 'Buddhism',\n",
       " 'Classics',\n",
       " 'Crime',\n",
       " 'Cultural',\n",
       " 'Emergency Services',\n",
       " 'Fantasy',\n",
       " 'Fiction',\n",
       " 'Humor',\n",
       " 'Memoir',\n",
       " 'Mystery',\n",
       " 'Mystery Thriller',\n",
       " 'Nonfiction',\n",
       " 'Novels',\n",
       " 'Paranormal',\n",
       " 'Philosophy',\n",
       " 'Police',\n",
       " 'Psychology',\n",
       " 'Religion',\n",
       " 'Spirituality',\n",
       " 'Supernatural',\n",
       " 'Suspense',\n",
       " 'Thriller'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_genres.intersection(rec_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b7d18",
   "metadata": {},
   "source": [
    "The model was able to pickup on a lot of user 55862's preferences such as, however it does miss others. A more detailed look at book ratings for the recommended and missed genres is required. Further, users with a lesser number of read books require analyzing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e9a5e",
   "metadata": {},
   "source": [
    "##### Packing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1ae7a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model\"\n",
    "version =  \"_v0.0.1\"\n",
    "zipped_model_name = \"zipped_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "24955a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_v0.0.1\\model_v0.0.1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_v0.0.1\\model_v0.0.1\\assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "# The use of the nested dirctory below is intended to ease zipping and unizpping later on\n",
    "tf.saved_model.save(indexer, os.path.join(f\"./{model_name}{version}\", f\"{model_name}{version}\"))\n",
    "\n",
    "path = os.path.join(\n",
    "            f\"{model_name}{version}\",\n",
    "            f\"{model_name}{version}\",\n",
    "            \"assets\",\n",
    "            \"placeholder\")\n",
    "open(path, 'a').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "43fa7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(os.path.join(f\"./{model_name}{version}\", f\"{model_name}{version}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "68bcc7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow_recommenders.layers.factorized_top_k.BruteForce,\n",
       " tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(indexer), type(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "11b5525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10753', '12683', '12921', '42062', '47722', '5963'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, titles = loaded(tf.constant([\"55862\"]))\n",
    "set([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f8f7a74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'18684', '28325', '32678', '44154', '5252'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, titles = indexer(tf.constant([\"703730\"]))\n",
    "set([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f230d78",
   "metadata": {},
   "source": [
    "Given the large model size, it's relevant to compress it for purposes of packaging since pruning using keras pruning tools wouldn't work on this custom tfrs Model. The target's to get it under 50 MiB to maintain on GitHub. Unzipping and loading the model takes a long time (500 ms) and does not make it suitable for live prediction meaning the model should be unzipped ideally as soon as the application goes live. Contanier orchastraion can facilitate the takedown and setup of pods where time delay in unzipping is no longer an encumbrance>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7003cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "def zip_directory(folder_path: str, zip_path:str, zip: bool = True, test: bool = False):\n",
    "    if zip:\n",
    "        with zipfile.ZipFile(zip_path, mode='w', compression=zipfile.ZIP_DEFLATED) as zipf:\n",
    "            len_dir_path = len(folder_path)\n",
    "            for root, _, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zipf.write(file_path, file_path[len_dir_path:])\n",
    "            zipf.close()\n",
    "            shutil.rmtree(folder_path)\n",
    "            if test:\n",
    "                result = (os.path.isdir(folder_path) and \n",
    "                          os.path.isfile(zip_path)) # false and true\n",
    "                return result == False and True\n",
    "            \n",
    "    else:\n",
    "        if test:\n",
    "            extract_path = os.mkdir(folder_path)\n",
    "        else:\n",
    "            extract_path = None\n",
    "        with zipfile.ZipFile(file=zip_path, mode=\"r\") as f:\n",
    "            f.extractall()\n",
    "            f.close()\n",
    "            os.remove(zip_path)\n",
    "            if test:\n",
    "                result = (os.path.isfile(zip_path) and \n",
    "                            os.path.isdir(folder_path)) # false and true\n",
    "                return result == False and True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e28473",
   "metadata": {},
   "source": [
    "Test zipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3b614059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_directory(folder_path=f\"./{model_name}{version}\", \n",
    "              zip_path=f\"{zipped_model_name}{version}.zip\",\n",
    "              zip=True,\n",
    "              test = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74d1c9",
   "metadata": {},
   "source": [
    "Test unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5d249e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_directory(folder_path=f\"./{model_name}{version}\", \n",
    "              zip_path=f\"{zipped_model_name}{version}.zip\",\n",
    "              zip=False,\n",
    "              test = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf50f0",
   "metadata": {},
   "source": [
    "Test the unzipped model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d71a84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(f\"{model_name}{version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7c86cc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'52727', '53615'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, titles = loaded(tf.constant([\"1\"]))\n",
    "set([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
