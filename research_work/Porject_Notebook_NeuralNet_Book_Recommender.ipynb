{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae282d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "import tensorflow_recommenders as tfrs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c2d22",
   "metadata": {},
   "source": [
    "Importing and organizing the raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0260233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# data = list()\n",
    "# for file in os.listdir(\"./data\"):\n",
    "##     if file != \"results_20000-24861.json\":\n",
    "#     with open(\"./data/\"+file, 'rb') as f:\n",
    "#         print(f\"Read {file}\")\n",
    "#         data_ = json.load(f)\n",
    "#         data += data_\n",
    "##     else:\n",
    "##         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee7f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# records = [{\"User\": record.get('user_id'), \n",
    "#            \"ISBN\": record.get('book_title'),\n",
    "#            \"RatingOf5\": record.get('stars'),\n",
    "#            \"Genres\": record.get('genres')\n",
    "#           } for record in data]\n",
    "# df = pd.DataFrame(records)\n",
    "# df.to_csv(\"USER_ISBN_RATING_GENRES.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e423556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../USER_ISBN_RATING_GENRES.csv\")\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bce96ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numer of unique books:  57198\n",
      "Total numer of unique users:  706880\n",
      "Total numer of reviews:  1733561\n",
      "Total numer of positive reviews:  936796\n",
      "Total numer of non-positive reviews:  201628\n",
      "Total numer of moderate reviews:  372374\n",
      "Total numer of  non-reviews:  222763\n"
     ]
    }
   ],
   "source": [
    "print(\"Total numer of unique books: \", len(df[\"ISBN\"].unique()))\n",
    "print(\"Total numer of unique users: \", len(df[\"User\"].unique()))\n",
    "print(\"Total numer of reviews: \", len(df))\n",
    "print(\"Total numer of positive reviews: \", len(df[df[\"RatingOf5\"] > 3]))\n",
    "print(\"Total numer of non-positive reviews: \", len(df[df[\"RatingOf5\"] < 3]))\n",
    "print(\"Total numer of moderate reviews: \", len(df[df[\"RatingOf5\"] == 3]))\n",
    "print(\"Total numer of  non-reviews: \", sum(df[\"RatingOf5\"].isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45c216",
   "metadata": {},
   "source": [
    "To keep users anonymous, I'll label encode them as IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129056e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1733561, 706880)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df[\"User\"])\n",
    "transformed = le.transform(df[\"User\"])\n",
    "len(transformed), len(np.unique(transformed, return_counts=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efc5427f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fitted_label_encoder.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(le, \"fitted_label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9afe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = joblib.load(\"fitted_label_encoder.pkl\")\n",
    "transformed = le.transform(df[\"User\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6649eba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    423114\n",
       "1    241063\n",
       "2    659623\n",
       "3    557019\n",
       "4     44930\n",
       "Name: User, dtype: int32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"User\"] = transformed\n",
    "df[\"User\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "848a364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf1817c",
   "metadata": {},
   "source": [
    "**I'm going to reserve a random sample of 10 observations to test recommendations as observations that the model has not seen before when developing the production package.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ea0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.groupby(\"ISBN\").count()[df.groupby(\"ISBN\").count()[\"User\"] > 5][\"User\"].sort_values(ascending=False).sample(20).index\n",
    "loop = True\n",
    "while True:\n",
    "    new_sample=df[df[\"ISBN\"].isin(sample)].sample(10)['ISBN']\n",
    "    if len(new_sample.unique()) == 10:\n",
    "        sample_to_keep = new_sample\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03f41a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df[df.index.isin(sample_to_keep.index)][['User', 'ISBN']]\n",
    "index_to_drop = sample.index\n",
    "sample.to_csv('test.csv', index=False)\n",
    "df.drop(index=index_to_drop, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24e9dd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>RatingOf5</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [User, ISBN, RatingOf5, Genres]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.index.isin(index_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb651465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>ISBN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36183</td>\n",
       "      <td>Voleurs de plage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>542310</td>\n",
       "      <td>Deadly Grounds Sydney Bryant 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>651722</td>\n",
       "      <td>Jumpin' Joe: The Jozef Sabovcik Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400575</td>\n",
       "      <td>The Church Cat Abroad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>267937</td>\n",
       "      <td>El hombre en el castillo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>496853</td>\n",
       "      <td>Core MySQL: The Serious Developer's Guide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>278028</td>\n",
       "      <td>Lord of Fire Lords and Ladies 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>202310</td>\n",
       "      <td>The Beans of Egypt Maine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>627962</td>\n",
       "      <td>Poet and Dancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>589726</td>\n",
       "      <td>Thinner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     User                                       ISBN\n",
       "0   36183                           Voleurs de plage\n",
       "1  542310             Deadly Grounds Sydney Bryant 2\n",
       "2  651722      Jumpin' Joe: The Jozef Sabovcik Story\n",
       "3  400575                      The Church Cat Abroad\n",
       "4  267937                   El hombre en el castillo\n",
       "5  496853  Core MySQL: The Serious Developer's Guide\n",
       "6  278028            Lord of Fire Lords and Ladies 1\n",
       "7  202310                   The Beans of Egypt Maine\n",
       "8  627962                            Poet and Dancer\n",
       "9  589726                                    Thinner"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1a738f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"User\", \"ISBN\"]].to_csv('train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a990170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>ISBN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423114</td>\n",
       "      <td>Kiss Hollywood Goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>241063</td>\n",
       "      <td>Kiss Hollywood Goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>659623</td>\n",
       "      <td>Kiss Hollywood Goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>557019</td>\n",
       "      <td>Kiss Hollywood Goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44930</td>\n",
       "      <td>Kiss Hollywood Goodbye</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     User                    ISBN\n",
       "0  423114  Kiss Hollywood Goodbye\n",
       "1  241063  Kiss Hollywood Goodbye\n",
       "2  659623  Kiss Hollywood Goodbye\n",
       "3  557019  Kiss Hollywood Goodbye\n",
       "4   44930  Kiss Hollywood Goodbye"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('train_data.csv', nrows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc6642",
   "metadata": {},
   "source": [
    "#### Reducing dimentionality\n",
    "\n",
    "With ~ 700k users and 60k books, the embeddings for the two embedding \"towers\" that the recommender model is going to be based on will generately an extremely large model. Practically, I would like to demonstrate the end-to-end data science pipeline by being able to package my model and make it available publicly for my API to access as well as for those interested which is why reduce the recommendations to those users who given more than 2 review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af78f7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User\n",
       "0         42\n",
       "7          3\n",
       "8          4\n",
       "25         3\n",
       "31         3\n",
       "          ..\n",
       "706863    12\n",
       "706867     6\n",
       "706870     6\n",
       "706871     4\n",
       "706879     4\n",
       "Name: ISBN, Length: 130887, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data = df.groupby(\"User\").count()\n",
    "grouped_data[grouped_data[\"ISBN\"] > 2]['ISBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9516c478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1733551"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbffddbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_keep = grouped_data[grouped_data[\"ISBN\"] > 2].index\n",
    "set(idx_to_keep) == set(df[df[\"User\"].isin(idx_to_keep)][\"User\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb70607",
   "metadata": {},
   "source": [
    "At this point we have an index of user that have more than 2 reviews (of different books). Our new DataFrame object will only comprise those users and their respectively reviewed books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e217c46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>ISBN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1245158</th>\n",
       "      <td>264027</td>\n",
       "      <td>High Fidelity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464505</th>\n",
       "      <td>479022</td>\n",
       "      <td>The Autobiography of Benjamin Franklin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76498</th>\n",
       "      <td>165464</td>\n",
       "      <td>The Fencing Master</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013314</th>\n",
       "      <td>139514</td>\n",
       "      <td>'Salem's Lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268727</th>\n",
       "      <td>550069</td>\n",
       "      <td>Salem Witch Trials: Alliance Project 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           User                                    ISBN\n",
       "1245158  264027                           High Fidelity\n",
       "464505   479022  The Autobiography of Benjamin Franklin\n",
       "76498    165464                      The Fencing Master\n",
       "1013314  139514                            'Salem's Lot\n",
       "268727   550069  Salem Witch Trials: Alliance Project 1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.User.isin(idx_to_keep)]\n",
    "df.sample(5)[['User', 'ISBN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a90e802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique users remaining:  130887\n",
      "# of unique books remaining:  56669\n"
     ]
    }
   ],
   "source": [
    "# also ensure that the grouping and querying was sane\n",
    "print(\"# of unique users remaining: \", len(df.groupby('User').count()))\n",
    "\n",
    "# also check out how many unique books were lost in the process of trimming \n",
    "# down the dataset (only about 500)\n",
    "\n",
    "print(\"# of unique books remaining: \", len(df.groupby('ISBN').count()))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99e941df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\AppData\\Local\\Temp\\ipykernel_22868\\42863754.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ISBN\"] = le_isbn.fit_transform(df[\"ISBN\"])\n"
     ]
    }
   ],
   "source": [
    "# we can label encode the book ID's if we so wish\n",
    "le_isbn = LabelEncoder()\n",
    "df[\"ISBN\"] = le_isbn.fit_transform(df[\"ISBN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348882f3",
   "metadata": {},
   "source": [
    "Setting up tensors to batch for purposes of training the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3689c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = tf.data.Dataset.from_tensor_slices(\n",
    "    df[\"ISBN\"].astype('str').values).shuffle(128) # TODO: config (shuffle seed + size)\n",
    "\n",
    "ratings = tf.data.Dataset.from_tensor_slices(\n",
    "    df[[\"ISBN\", \"User\"]].astype('str').values).shuffle(128) # TODO: config (shuffle seed + size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fb66023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['22378', '423114'],\n",
       " ['50409', '264027'],\n",
       " ['51897', '216574'],\n",
       " ['50409', '377761'],\n",
       " ['50409', '111376'],\n",
       " ['32867', '111877'],\n",
       " ['22378', '36177'],\n",
       " ['14058', '670849'],\n",
       " ['22378', '349993'],\n",
       " ['25405', '285275']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in ratings.batch(batch_size=10).take(1):\n",
    "    break\n",
    "\n",
    "# [[Book title, User ID],]\n",
    "[[batch[i].numpy()[j].decode() for j in range(2)] for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae1f6b",
   "metadata": {},
   "source": [
    "String look up creates a vocabulary dictionary of string-value pairs since I'll be embedding my ISBN's and users ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c94c80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x: x[1]))\n",
    "\n",
    "book_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "book_titles_vocabulary.adapt(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "411626e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"user_ids_vocabulary\", user_ids_vocabulary.get_weights())\n",
    "np.save(\"book_titles_vocabulary\", book_titles_vocabulary.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0f74510",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_book_title_stringlookup = tf.keras.layers.StringLookup(mask_token=None)\n",
    "new_book_title_stringlookup.set_weights(np.load(\"book_titles_vocabulary.npy\", allow_pickle=True))\n",
    "new_user_id_stringlookup = tf.keras.layers.StringLookup(mask_token=None)\n",
    "new_user_id_stringlookup.set_weights(np.load(\"user_ids_vocabulary.npy\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8896f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique User ID counts:  130887 130888\n",
      "Unique ISBN counts:  56669 56670\n"
     ]
    }
   ],
   "source": [
    "# compare vocab against original data as a sanity check\n",
    "print(\"Unique User ID counts: \", len(df[\"User\"].unique()), new_user_id_stringlookup.vocabulary_size())\n",
    "print(\"Unique ISBN counts: \", len(df[\"ISBN\"].unique()), new_book_title_stringlookup.vocabulary_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f100ec6d",
   "metadata": {},
   "source": [
    "The difference in \"vocab\" counts results from the [UNK] (i.e. unknown) token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4276c",
   "metadata": {},
   "source": [
    "Testing stringlookup on the saved sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e99c6e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56670"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_book_title_stringlookup.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "206e0238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)>,\n",
       " 94045                               Voleurs de plage\n",
       " 587383                Deadly Grounds Sydney Bryant 2\n",
       " 812079         Jumpin' Joe: The Jozef Sabovcik Story\n",
       " 1088663                        The Church Cat Abroad\n",
       " 1309076                     El hombre en el castillo\n",
       " 1413731    Core MySQL: The Serious Developer's Guide\n",
       " 1636847              Lord of Fire Lords and Ladies 1\n",
       " 1691249                     The Beans of Egypt Maine\n",
       " 1716389                              Poet and Dancer\n",
       " 1728612                                      Thinner\n",
       " Name: ISBN, dtype: object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_book_title_stringlookup.call(tf.constant(sample['ISBN'].astype(str))), sample['ISBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b54208e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=int64, numpy=\n",
       " array([110217,  22004,      0,      0,    256,      0,  11467,      0,\n",
       "         30130,    347], dtype=int64)>,\n",
       " 94045       36183\n",
       " 587383     542310\n",
       " 812079     651722\n",
       " 1088663    400575\n",
       " 1309076    267937\n",
       " 1413731    496853\n",
       " 1636847    278028\n",
       " 1691249    202310\n",
       " 1716389    627962\n",
       " 1728612    589726\n",
       " Name: User, dtype: int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_id_stringlookup.call(tf.constant(sample['User'].astype(str))), sample['User']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0c9cc",
   "metadata": {},
   "source": [
    "Several user_ids have a string lookup of 0 which stands for unknown meaning they were likely dropped during the attemp at dimentionality reduction (i.e. these users had only ever reviewed one/two book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63866a0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m idx \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# this is a bit of  confusing one-liner that simply attempts to search for the indices of users that \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# had the string lookup assign them a value of 0 and thus are the suspicions confirmed that\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# they've been dropped during the dimentionality reduction step since they're users with less than \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 2 total reviews.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df[df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(sample[sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(idx)]\u001b[38;5;241m.\u001b[39mindex)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx = [0, 3, 8]\n",
    "\n",
    "# this is a bit of  confusing one-liner that simply attempts to search for the indices of users that \n",
    "# had the string lookup assign them a value of 0 and thus are the suspicions confirmed that\n",
    "# they've been dropped during the dimentionality reduction step since they're users with less than \n",
    "# 2 total reviews.\n",
    "\n",
    "assert len(df[df.index.isin(sample[sample['User'].reset_index().index.isin(idx)].index)]['User'].unique()) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873a44d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "I'll be doing tests for label encoders and string lookups on the books columns and not the users columns before deployment \n",
    "since their info is confidential, otherwise, for testing purposes, the sample taken isn't valid and would fail the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9e09f",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d073e7d7",
   "metadata": {},
   "source": [
    "The two tower model has user features and movie features as embeddings and uses implicit assumptions as in books read being a positive example and those not read being an negative one. TFRS Retrieval handles this through a top K factorization which compares accuracy `tf.keras.metrics.TopKCategoricalAccuracy(k=K)` of books predicted to be by the queries user vs actual books read.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2853d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024*8\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7d4d98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(tfrs.Model):\n",
    "    def __init__(\n",
    "      self,\n",
    "      user_model: tf.keras.Model,\n",
    "      book_model: tf.keras.Model,\n",
    "      task: tfrs.tasks.Retrieval):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "    \n",
    "        # embedding representations\n",
    "        self.user_model = user_model\n",
    "        self.book_model = book_model\n",
    "        # this will receive the computed losses by using the retrieval class to calculate the FactorizedTopK\n",
    "        self.task = task \n",
    "    \n",
    "    def compute_loss(self, features, training=False) -> tf.Tensor:\n",
    "        # Define how the loss is computed.\n",
    "        user_embeddings = self.user_model(features[:, 1])\n",
    "        book_embeddings = self.book_model(features[:, 0])\n",
    "        \n",
    "#         print(user_embeddings.shape)\n",
    "        \n",
    "        return self.task(user_embeddings, book_embeddings, compute_metrics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c634ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "    new_user_id_stringlookup,\n",
    "    tf.keras.layers.Embedding(new_user_id_stringlookup.vocabulary_size()+1 , 32)\n",
    "])\n",
    "\n",
    "book_model = tf.keras.Sequential([\n",
    "    new_book_title_stringlookup,\n",
    "    tf.keras.layers.Embedding(new_book_title_stringlookup.vocabulary_size()+1, 32)\n",
    "])\n",
    "\n",
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "    books.batch(256).map(book_model)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "537c0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_model = TwoTowerModel(user_model, book_model, task)\n",
    "tt_model.compile(optimizer=tf.keras.optimizers.Adam(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7531155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'14058' b'670849'], shape=(2,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for i in ratings.take(1):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "65bd120e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "129/129 [==============================] - 4s 29ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 75616.6461 - regularization_loss: 0.0000e+00 - total_loss: 75616.6461\n",
      "Epoch 2/10\n",
      "129/129 [==============================] - 4s 29ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 101869.5806 - regularization_loss: 0.0000e+00 - total_loss: 101869.5806\n",
      "Epoch 3/10\n",
      "129/129 [==============================] - 4s 29ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 106330.1353 - regularization_loss: 0.0000e+00 - total_loss: 106330.1353\n",
      "Epoch 4/10\n",
      "129/129 [==============================] - 4s 29ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 94618.1675 - regularization_loss: 0.0000e+00 - total_loss: 94618.1675\n",
      "Epoch 5/10\n",
      "129/129 [==============================] - 4s 29ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 102189.2103 - regularization_loss: 0.0000e+00 - total_loss: 102189.2103\n",
      "Epoch 6/10\n",
      "129/129 [==============================] - 4s 29ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 111907.3840 - regularization_loss: 0.0000e+00 - total_loss: 111907.3840\n",
      "Epoch 7/10\n",
      "129/129 [==============================] - 4s 29ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 114568.4547 - regularization_loss: 0.0000e+00 - total_loss: 114568.4547\n",
      "Epoch 8/10\n",
      "129/129 [==============================] - 4s 29ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 113944.1597 - regularization_loss: 0.0000e+00 - total_loss: 113944.1597\n",
      "Epoch 9/10\n",
      "129/129 [==============================] - 4s 29ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 112006.2809 - regularization_loss: 0.0000e+00 - total_loss: 112006.2809\n",
      "Epoch 10/10\n",
      "129/129 [==============================] - 4s 29ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 109331.2229 - regularization_loss: 0.0000e+00 - total_loss: 109331.2229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c29d153e20>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_model.fit(ratings.batch(BATCH_SIZE), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ba059",
   "metadata": {},
   "source": [
    "The brute force approach goes through every index to get the top K recommendation. This is considered tedious and the alternative scalable nearest neighbours approach provides a viable alternative since the latter utilizes compressed vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "96c58ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x2c29d153af0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = tfrs.layers.factorized_top_k.BruteForce(tt_model.user_model, k = 100)\n",
    "indexer.index_from_dataset(\n",
    "    books.batch(64).map(lambda title: (title, tt_model.book_model(title))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474dac1",
   "metadata": {},
   "source": [
    "Let's get some recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4fe8bc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 8]\n",
      "[ 36183 542310 651722 400575 267937 496853 278028 202310 627962 589726]\n"
     ]
    }
   ],
   "source": [
    "print(idx) # users that the string lookupand hence model doesn't know and should give then random recommendations\n",
    "print(sample['User'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "91f8b6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['15006', '40107', '40442', '52155', '53676'], dtype='<U5')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = indexer(tf.constant([\"703730\"]))\n",
    "_, titles = indexer(tf.constant([\"703730\"]))\n",
    "# the 5 unique recommendations indicates the retriever model along isn't enough\n",
    "np.unique([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a4d08804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ms ± 19.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit _, titles = indexer(tf.constant([\"3\"]))\n",
    "_, titles = indexer(tf.constant([\"3\"]))\n",
    "set([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e1945",
   "metadata": {},
   "source": [
    "The time taken is quite slow for a live model which is why a ScaNN alogorithm can be used which approximates recommendations user a nearest-neighbours approach.\n",
    "\n",
    "As a sanity check, I'll get a couple of unknow users' recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aaa0e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\"683095\" or \"47561\") not in new_user_id_stringlookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b75f52cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683095 :  {'32926', '21845', '11710', '42079', '30917', '4823', '20803', '54757'}\n",
      "47561  :  {'32926', '21845', '11710', '42079', '30917', '4823', '20803', '54757'}\n"
     ]
    }
   ],
   "source": [
    "for user in ['683095', '47561 ']:\n",
    "    _, titles = indexer(tf.constant([user]))\n",
    "    print(user, \": \", set([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5cd9e",
   "metadata": {},
   "source": [
    "The similarity in recommendations to unknown indicates that all unknown users are given the same random recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f77917",
   "metadata": {},
   "source": [
    "**Making sense of the recommendations requires analyzing the genres of book user \"100023\" read and liked.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a6e9baa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gothic',\n",
       " 'Dogs',\n",
       " 'Adventure',\n",
       " 'Suspense',\n",
       " 'Young Adult',\n",
       " 'Womens',\n",
       " 'Banned Books',\n",
       " 'How To',\n",
       " 'Nonfiction',\n",
       " 'Crime',\n",
       " 'Mystery Thriller',\n",
       " 'Family',\n",
       " 'Memoir',\n",
       " 'Short Story Collection',\n",
       " 'Womens Fiction',\n",
       " 'Ireland',\n",
       " 'Poetry',\n",
       " 'Philosophy',\n",
       " 'Adult',\n",
       " 'Health',\n",
       " 'Outdoors',\n",
       " 'The United States Of America',\n",
       " 'European Literature',\n",
       " '20th Century',\n",
       " 'Supernatural',\n",
       " 'Detective',\n",
       " 'France',\n",
       " 'Personal Development',\n",
       " 'Nature',\n",
       " 'Witches',\n",
       " 'Fiction',\n",
       " 'Kids',\n",
       " 'Scotland',\n",
       " 'Book Club',\n",
       " 'Cultural',\n",
       " 'Realistic Fiction',\n",
       " 'Young Readers',\n",
       " 'Canada',\n",
       " 'Paranormal',\n",
       " 'Modern Classics',\n",
       " 'Animals',\n",
       " 'Academic',\n",
       " 'Travel',\n",
       " 'Coming Of Age',\n",
       " 'Female Authors',\n",
       " 'Chapter Books',\n",
       " 'School',\n",
       " 'Gothic Romance',\n",
       " 'Short Stories',\n",
       " 'Psychological Thriller',\n",
       " 'Juvenile',\n",
       " 'Cozy Mystery',\n",
       " 'Emergency Services',\n",
       " 'Audiobook',\n",
       " 'Spirituality',\n",
       " 'Self Help',\n",
       " 'Childrens',\n",
       " 'Historical',\n",
       " 'Autobiography',\n",
       " 'Diets',\n",
       " 'Psychology',\n",
       " 'Mystery',\n",
       " 'Biography Memoir',\n",
       " 'Romantic Suspense',\n",
       " 'Biography',\n",
       " 'Classics',\n",
       " 'Contemporary',\n",
       " 'Judaism',\n",
       " 'Buddhism',\n",
       " 'Romance',\n",
       " 'Novels',\n",
       " 'Literature',\n",
       " 'Irish Literature',\n",
       " 'Literary Fiction',\n",
       " 'British Literature',\n",
       " 'Feminism',\n",
       " 'Middle Grade',\n",
       " 'Humor',\n",
       " 'Religion',\n",
       " 'Canadian Literature',\n",
       " 'History',\n",
       " 'Historical Fiction',\n",
       " 'Environment',\n",
       " 'Relationships',\n",
       " 'Police',\n",
       " 'Jewish',\n",
       " 'Adult Fiction',\n",
       " 'Food and Drink',\n",
       " 'Murder Mystery',\n",
       " 'China',\n",
       " 'American',\n",
       " 'Thriller',\n",
       " 'Fantasy']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_genres = set()\n",
    "user_read_genres = df2[df2[\"User\"]==55862][\"Genres\"].unique()\n",
    "user_read_genres = [ast.literal_eval(user_read_genres[i]) for i in range(len(user_read_genres))];\n",
    "[[read_genres.add(i) for i in element] for element in user_read_genres];\n",
    "list(read_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d07fc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recommended_books = ['Brennen muss Salem',\n",
    " 'How to Know God: The Yoga Aphorisms of Patanjali',\n",
    " \"It's Not Mean If It's True: More Trials from My Queer Life\",\n",
    " 'Night Judgement At Sinos A Novel',\n",
    " 'The House on the Point: A Tribute to Franklin W Dixon and The Hardy Boys',\n",
    " 'Tularosa Kevin Kerney 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "68cdf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = df2[df2[\"ISBN\"].isin(user_recommended_books)]['Genres'].unique()\n",
    "recommendations = [ast.literal_eval(recommendations[i]) for i in range(len(recommendations))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a7c86487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Action',\n",
       " 'Adult',\n",
       " 'Adventure',\n",
       " 'Audiobook',\n",
       " 'Autobiography',\n",
       " 'Buddhism',\n",
       " 'Classics',\n",
       " 'Crime',\n",
       " 'Cultural',\n",
       " 'Eastern Philosophy',\n",
       " 'Emergency Services',\n",
       " 'Espionage',\n",
       " 'Essays',\n",
       " 'Fantasy',\n",
       " 'Fiction',\n",
       " 'Gay',\n",
       " 'Hinduism',\n",
       " 'Horror',\n",
       " 'Humor',\n",
       " 'India',\n",
       " 'LGBT',\n",
       " 'Memoir',\n",
       " 'Mystery',\n",
       " 'Mystery Thriller',\n",
       " 'Nonfiction',\n",
       " 'Novels',\n",
       " 'Paranormal',\n",
       " 'Philosophy',\n",
       " 'Police',\n",
       " 'Psychology',\n",
       " 'Queer',\n",
       " 'Religion',\n",
       " 'Spirituality',\n",
       " 'Spy Thriller',\n",
       " 'Supernatural',\n",
       " 'Suspense',\n",
       " 'Thriller',\n",
       " 'Vampires',\n",
       " 'Westerns',\n",
       " 'Writing'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_genres = set()\n",
    "[[rec_genres.add(i) for i in element] for element in recommendations];\n",
    "rec_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fe0ddc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adult',\n",
       " 'Adventure',\n",
       " 'Audiobook',\n",
       " 'Autobiography',\n",
       " 'Buddhism',\n",
       " 'Classics',\n",
       " 'Crime',\n",
       " 'Cultural',\n",
       " 'Emergency Services',\n",
       " 'Fantasy',\n",
       " 'Fiction',\n",
       " 'Humor',\n",
       " 'Memoir',\n",
       " 'Mystery',\n",
       " 'Mystery Thriller',\n",
       " 'Nonfiction',\n",
       " 'Novels',\n",
       " 'Paranormal',\n",
       " 'Philosophy',\n",
       " 'Police',\n",
       " 'Psychology',\n",
       " 'Religion',\n",
       " 'Spirituality',\n",
       " 'Supernatural',\n",
       " 'Suspense',\n",
       " 'Thriller'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_genres.intersection(rec_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b7d18",
   "metadata": {},
   "source": [
    "The model was able to pickup on a lot of user 55862's preferences such as, however it does miss others. A more detailed look at book ratings for the recommended and missed genres is required. Further, users with a lesser number of read books require analyzing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e9a5e",
   "metadata": {},
   "source": [
    "##### Packing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1ae7a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model\"\n",
    "version =  \"_v0.0.1\"\n",
    "zipped_model_name = \"zipped_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "24955a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_v0.0.1\\model_v0.0.1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_v0.0.1\\model_v0.0.1\\assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "# The use of the nested dirctory below is intended to ease zipping and unizpping later on\n",
    "tf.saved_model.save(indexer, os.path.join(f\"./{model_name}{version}\", f\"{model_name}{version}\"))\n",
    "\n",
    "path = os.path.join(\n",
    "            f\"{model_name}{version}\",\n",
    "            f\"{model_name}{version}\",\n",
    "            \"assets\",\n",
    "            \"placeholder\")\n",
    "open(path, 'a').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "43fa7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(os.path.join(f\"./{model_name}{version}\", f\"{model_name}{version}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "68bcc7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow_recommenders.layers.factorized_top_k.BruteForce,\n",
       " tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(indexer), type(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "11b5525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10753', '12683', '12921', '42062', '47722', '5963'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, titles = loaded(tf.constant([\"55862\"]))\n",
    "set([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f8f7a74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'18684', '28325', '32678', '44154', '5252'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, titles = indexer(tf.constant([\"703730\"]))\n",
    "set([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f230d78",
   "metadata": {},
   "source": [
    "Given the large model size, it's relevant to compress it for purposes of packaging since pruning using keras pruning tools wouldn't work on this custom tfrs Model. The target's to get it under 50 MiB to maintain on GitHub. Unzipping and loading the model takes a long time (500 ms) and does not make it suitable for live prediction meaning the model should be unzipped ideally as soon as the application goes live. Contanier orchastraion can facilitate the takedown and setup of pods where time delay in unzipping is no longer an encumbrance>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7003cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "def zip_directory(folder_path: str, zip_path:str, zip: bool = True, test: bool = False):\n",
    "    if zip:\n",
    "        with zipfile.ZipFile(zip_path, mode='w', compression=zipfile.ZIP_DEFLATED) as zipf:\n",
    "            len_dir_path = len(folder_path)\n",
    "            for root, _, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zipf.write(file_path, file_path[len_dir_path:])\n",
    "            zipf.close()\n",
    "            shutil.rmtree(folder_path)\n",
    "            if test:\n",
    "                result = (os.path.isdir(folder_path) and \n",
    "                          os.path.isfile(zip_path)) # false and true\n",
    "                return result == False and True\n",
    "            \n",
    "    else:\n",
    "        if test:\n",
    "            extract_path = os.mkdir(folder_path)\n",
    "        else:\n",
    "            extract_path = None\n",
    "        with zipfile.ZipFile(file=zip_path, mode=\"r\") as f:\n",
    "            f.extractall()\n",
    "            f.close()\n",
    "            os.remove(zip_path)\n",
    "            if test:\n",
    "                result = (os.path.isfile(zip_path) and \n",
    "                            os.path.isdir(folder_path)) # false and true\n",
    "                return result == False and True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e28473",
   "metadata": {},
   "source": [
    "Test zipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3b614059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_directory(folder_path=f\"./{model_name}{version}\", \n",
    "              zip_path=f\"{zipped_model_name}{version}.zip\",\n",
    "              zip=True,\n",
    "              test = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74d1c9",
   "metadata": {},
   "source": [
    "Test unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5d249e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_directory(folder_path=f\"./{model_name}{version}\", \n",
    "              zip_path=f\"{zipped_model_name}{version}.zip\",\n",
    "              zip=False,\n",
    "              test = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf50f0",
   "metadata": {},
   "source": [
    "Test the unzipped model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d71a84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(f\"{model_name}{version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7c86cc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'52727', '53615'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, titles = loaded(tf.constant([\"1\"]))\n",
    "set([titles.numpy()[0][i].decode() for i in range(len(titles.numpy()[0]))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
